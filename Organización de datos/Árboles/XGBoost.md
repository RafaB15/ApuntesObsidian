- Boosting de √°rboles de decisi√≥n.
- Tiene una funci√≥n objetivo
    
    $$
    Obj(\theta) = L(\theta) + \Omega(\theta)
    $$
    
    $\theta:$ par√°metro a aprender
    
    $L:$ error del modelo
    
    $\Omega:$ factor de regularizaci√≥n
    

$$
L(\theta) = \sum_{i=1}^nl(y_i, \hat y_i)
$$

$l:$ es el error en la predicci√≥n de Y.

Para regresi√≥n puede ser ‚Üí $l = (y_i - \hat y _i )^2$

Para clasificaci√≥n binaria ‚Üí $l = y_i \ln(1+ e^{- \hat y_i})+(1-y_i)\ln(i+e^{\hat y_i})$

- La predicci√≥n es la sumatoria de la predicci√≥n de varios √°rboles

$$
\hat y_i = \sum_{j=1}^k f_j(x_i)
$$

![[Organizaci√≥n de datos/√Årboles/XGBoost/Untitled.png|Untitled]]

![[Organizaci√≥n de datos/√Årboles/XGBoost/Untitled 1.png|Untitled]]

![[Organizaci√≥n de datos/√Årboles/XGBoost/Untitled 2.png|Untitled]]

![[Organizaci√≥n de datos/√Årboles/XGBoost/Untitled 3.png|Untitled]]

![[Organizaci√≥n de datos/√Årboles/XGBoost/Untitled 4.png|Untitled]]

![[Organizaci√≥n de datos/√Årboles/XGBoost/Untitled 5.png|Untitled]]

![[Organizaci√≥n de datos/√Årboles/XGBoost/Untitled 6.png|Untitled]]

![[Organizaci√≥n de datos/√Årboles/XGBoost/Untitled 7.png|Untitled]]

![[Organizaci√≥n de datos/√Årboles/XGBoost/Untitled 8.png|Untitled]]

![[Organizaci√≥n de datos/√Årboles/XGBoost/Untitled 9.png|Untitled]]

![[Organizaci√≥n de datos/√Årboles/XGBoost/Untitled 10.png|Untitled]]

![[Organizaci√≥n de datos/√Årboles/XGBoost/Untitled 11.png|Untitled]]

![[Organizaci√≥n de datos/√Årboles/XGBoost/Untitled 12.png|Untitled]]

# Conclusi√≥n:

XGBoosting hace varios √°rboles, uno predice bien algunas cosas, entonces hace una predicci√≥n y cuando le da bien lo dejamos ah√≠, pero cuando le da mal se lo pasa a otro √°rbol para ver si ese predice bien. Algo as√≠ üòõ